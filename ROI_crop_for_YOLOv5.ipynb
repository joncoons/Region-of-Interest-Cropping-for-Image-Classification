{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --pre --upgrade azure-ai-ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "!pip install opencv_contrib_python~=4.5.5 torch==1.8.1 torchvision==0.9.1 azure-storage-blob==12.8.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_project = True\n",
        "video_project = False\n",
        "minimum_crop_size = 0\n",
        "azure_automl_name = '<automl_name_of_your_job>'\n",
        "azure_storage_connection_string = \"<your_azure_storage_connection_string>\"\n",
        "azure_storage_container_input = \"<container_name_for_raw_images>\"\n",
        "azure_storage_container_prefix=\"<prefix_for_raw_images>\" # e.g. \"\" if none or \"folder1/folder2/\"\n",
        "azure_storage_container_output = \"<container_name_for_cropped_images>\" # only lower case letters, nubers and dashes\n",
        "local_directory_name = \"<directory_name_for_aml_artifacts>\" # e.g. \"aml_artifacts\"\n",
        "\n",
        "# Optional - if using captured video\n",
        "azure_sas_uri_for_video = \"<your_azure_sas_uri_for_video>\" # e.g. \"https://<storage_account_name>.blob.core.windows.net/<container_name>/<video_name>?<sas_token>\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import job information and artifacts from AML, including the ONNX file from the training artifacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "ml_client = None\n",
        "try:\n",
        "    ml_client = MLClient.from_config(credential)\n",
        "except Exception as ex:\n",
        "    print(ex)\n",
        "    # Enter details of your AML workspace\n",
        "    subscription_id = ''   \n",
        "    resource_group = ''  \n",
        "    workspace_name = ''\n",
        "    ml_client = MLClient(credential, subscription_id, resource_group, workspace_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "from mlflow.tracking.client import MlflowClient\n",
        "\n",
        "# Had to add to documentation\n",
        "mlflow_client = MlflowClient()\n",
        "\n",
        "# Obtain the tracking URL from MLClient\n",
        "MLFLOW_TRACKING_URI = ml_client.workspaces.get(\n",
        "    name=ml_client.workspace_name\n",
        ").mlflow_tracking_uri\n",
        "\n",
        "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "\n",
        "# Specify the job name\n",
        "job_name = azure_automl_name\n",
        "\n",
        "# Get the parent run\n",
        "mlflow_parent_run = mlflow_client.get_run(job_name)\n",
        "best_child_run_id = mlflow_parent_run.data.tags['automl_best_child_run_id']\n",
        "# get the best child run\n",
        "best_run = mlflow_client.get_run(best_child_run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "local_dir = local_directory_name\n",
        "if not os.path.exists(local_dir):\n",
        "    os.mkdir(local_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "labels_file = mlflow_client.download_artifacts(\n",
        "    best_run.info.run_id, 'train_artifacts/labels.json', local_dir\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "onnx_model_path = mlflow_client.download_artifacts(\n",
        "    best_run.info.run_id, 'train_artifacts/model.onnx', local_dir\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "conda_file = mlflow_client.download_artifacts(\n",
        "    best_run.info.run_id, 'outputs/conda_env_v_1_0_0.yml', local_dir\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "env = Environment(\n",
        "    name=\"automl-images-env-onnx\",\n",
        "    description=\"environment for automl images ONNX batch model generation\",\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04\",\n",
        "    conda_file=conda_file,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load model for inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import onnxruntime\n",
        "\n",
        "with open(labels_file) as f:\n",
        "    classes = json.load(f)\n",
        "print(classes)\n",
        "try:\n",
        "    session = onnxruntime.InferenceSession(onnx_model_path)\n",
        "    print(\"ONNX model loaded...\")\n",
        "except Exception as e: \n",
        "    print(\"Error loading ONNX file: \", str(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "sess_input = session.get_inputs()\n",
        "sess_output = session.get_outputs()\n",
        "print(f\"No. of inputs : {len(sess_input)}, No. of outputs : {len(sess_output)}\")\n",
        "\n",
        "for idx, input_ in enumerate(range(len(sess_input))):\n",
        "    input_name = sess_input[input_].name\n",
        "    input_shape = sess_input[input_].shape\n",
        "    input_type = sess_input[input_].type\n",
        "    print(f\"{idx} Input name : { input_name }, Input shape : {input_shape}, \\\n",
        "    Input type  : {input_type}\")  \n",
        "\n",
        "for idx, output in enumerate(range(len(sess_output))):\n",
        "    output_name = sess_output[output].name\n",
        "    output_shape = sess_output[output].shape\n",
        "    output_type = sess_output[output].type\n",
        "    print(f\" {idx} Output name : {output_name}, Output shape : {output_shape}, \\\n",
        "    Output type  : {output_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "batch, channel, height_onnx, width_onnx = session.get_inputs()[0].shape\n",
        "batch, channel, height_onnx, width_onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.storage.blob import BlobServiceClient, ContainerClient, BlobClient, StorageStreamDownloader\n",
        "\n",
        "from azure.core.exceptions import ResourceExistsError\n",
        "\n",
        "storage_client = BlobServiceClient.from_connection_string(azure_storage_connection_string)\n",
        "\n",
        "try:\n",
        "    input_container_client = storage_client.create_container(azure_storage_container_input)\n",
        "        \n",
        "except ResourceExistsError as e:\n",
        "    print(e)\n",
        "    input_container_client = storage_client.get_container_client(azure_storage_container_input)\n",
        "\n",
        "try:\n",
        "    output_container_client = storage_client.create_container(azure_storage_container_output)\n",
        "        \n",
        "except ResourceExistsError as e:\n",
        "    print(e)\n",
        "    output_container_client = storage_client.get_container_client(azure_storage_container_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# sys.path.append(r'/home/azureuser/cloudfiles/code/Users/<user name>/<path to notebook>')\n",
        "from onnxruntime_yolov5 import initialize_yolov5\n",
        "\n",
        "target_dim = 640\n",
        "target_iou = .45\n",
        "target_prob = .50\n",
        "\n",
        "initialize_yolov5(onnx_model_path, labels_file, target_dim, target_prob, target_iou)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_extension(filename):\n",
        "    file_extensions = set(['png', 'jpg', 'jpeg', 'bmp', 'tiff', 'tif'])\n",
        "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in file_extensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.9.13 64-bit' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Users/jocoons/AppData/Local/Programs/Python/Python39/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import io\n",
        "import cv2\n",
        "import time\n",
        "import json\n",
        "import uuid\n",
        "import numpy as np\n",
        "from frame_preprocess import frame_resize\n",
        "from onnxruntime_yolov5 import predict_yolov5\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "frame_count = 0\n",
        "crop_count = 0\n",
        "\n",
        "if image_project:\n",
        "    images = input_container_client.list_blobs(name_starts_with=azure_storage_container_prefix)\n",
        "    for blob in images:\n",
        "        image_name = blob.name\n",
        "        if check_extension(image_name):\n",
        "            frame_count += 1\n",
        "            blob_client = input_container_client.get_blob_client(blob.name)\n",
        "            blob_stream = blob_client.download_blob()\n",
        "            blob_bytes = blob_stream.readall()\n",
        "            frame = Image.open(io.BytesIO(blob_bytes))\n",
        "            frame = np.array(frame)\n",
        "            h, w = frame.shape[:2]\n",
        "            print(f\"Frame size: {w} x {h}\")\n",
        "            frame_optimized, ratio, pad_list = frame_resize(frame, target_dim, model = \"yolov5\")\n",
        "            result = predict_yolov5(frame_optimized, pad_list)\n",
        "            predictions = result['predictions'][0]\n",
        "\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            \n",
        "            # if result is not None:\n",
        "                # print(json.dumps(result))\n",
        "\n",
        "            if predictions is not None:\n",
        "                detection_count = len(predictions)\n",
        "                t_infer = result[\"inference_time\"]\n",
        "                # print(f\"Inference Time: {t_infer}ms\")\n",
        "                # print(f\"Detection Count: {detection_count}\")\n",
        "\n",
        "            now = datetime.now()\n",
        "            created = now.isoformat()\n",
        "\n",
        "            if detection_count > 0:\n",
        "                for i in range(detection_count):\n",
        "                    unique_id = str(uuid.uuid4())\n",
        "                    crop_name = f\"{created}-{frame_count}-{unique_id}.jpg\"\n",
        "                    bounding_box = predictions[i]['bbox']\n",
        "                    tag_name = predictions[i]['labelName']\n",
        "                    object_crop = frame[int(bounding_box[\"top\"]/ratio[1]): int(bounding_box[\"height\"]/ratio[1]), int(bounding_box[\"left\"]/ratio[0]): int(bounding_box[\"width\"]/ratio[0])]\n",
        "                    \n",
        "                    # increase crop size by 100 in each direction\n",
        "                    # object_crop = frame[int(bounding_box[\"top\"]/ratio[1]-100): int(bounding_box[\"height\"]/ratio[1]+100), int(bounding_box[\"left\"]/ratio[0]-100): int(bounding_box[\"width\"]/ratio[0]+100)]\n",
        "                    \n",
        "                    object_h, object_w = object_crop.shape[:2]\n",
        "                    print(f\"Object width x height: {object_w} x {object_h}\")\n",
        "\n",
        "                    if minimum_crop_size > 0:\n",
        "                        if object_h and object_w >= minimum_crop_size:\n",
        "                            crop_count += 1\n",
        "                            encoded_crop = cv2.imencode(\".jpg\", object_crop)[1].tobytes()\n",
        "                            try:\n",
        "                                blob_client = output_container_client.get_blob_client(f\"{tag_name}/{crop_name}\")\n",
        "                                blob_client.upload_blob(encoded_crop, blob_type=\"BlockBlob\")\n",
        "                                print(f\"Successfully uploaded {crop_name}\")\n",
        "                                # plt.imshow(object_crop)\n",
        "                                # plt.show()\n",
        "                            except Exception as e:\n",
        "                                print(e)\n",
        "\n",
        "                    else:\n",
        "                        crop_count += 1\n",
        "                        encoded_crop = cv2.imencode(\".jpg\", object_crop)[1].tobytes()\n",
        "                        try:\n",
        "                            blob_client = output_container_client.get_blob_client(f\"{tag_name}/{crop_name}\")\n",
        "                            blob_client.upload_blob(encoded_crop, blob_type=\"BlockBlob\")\n",
        "                            print(f\"Successfully uploaded {crop_name}\")\n",
        "                            # plt.imshow(object_crop)\n",
        "                            # plt.show()\n",
        "                        except Exception as e:\n",
        "                            print(e)\n",
        "\n",
        "elif video_project:\n",
        "    frame_rate_count = 0\n",
        "    camera_fps = 30\n",
        "    inference_fps = 1\n",
        "    vid_cap = cv2.VideoCapture(azure_sas_uri_for_video)\n",
        "    while vid_cap.isOpened():\n",
        "        _, frame = vid_cap.read()\n",
        "        if _ == True:\n",
        "            frame_count += 1\n",
        "            frame_rate_count += 1\n",
        "            if inference_fps > 0:\n",
        "                if frame_rate_count == int(camera_fps/inference_fps):\n",
        "                    frame_rate_count = 0\n",
        "                    h, w = frame.shape[:2]\n",
        "                    frame_optimized, ratio, pad_list = frame_resize(frame, target_dim, model = \"yolov5\")\n",
        "                    result = predict_yolov5(frame_optimized, pad_list)\n",
        "                    predictions = result['predictions'][0]\n",
        "                    \n",
        "                    # if result is not None:\n",
        "                        # print(json.dumps(result))\n",
        "\n",
        "                    if predictions is not None:\n",
        "                        detection_count = len(predictions)\n",
        "                        t_infer = result[\"inference_time\"]\n",
        "                        # print(f\"Inference Time: {t_infer}ms\")\n",
        "                        # print(f\"Detection Count: {detection_count}\")\n",
        "\n",
        "                    now = datetime.now()\n",
        "                    created = now.isoformat()\n",
        "\n",
        "                    if detection_count > 0:\n",
        "                        for i in range(detection_count):\n",
        "                            unique_id = str(uuid.uuid4())\n",
        "                            crop_name = f\"{created}-{frame_count}-{unique_id}.jpg\"\n",
        "                            bounding_box = predictions[i]['bbox']\n",
        "                            tag_name = predictions[i]['labelName']\n",
        "                            object_crop = frame[int(bounding_box[\"top\"]/ratio[1]): int(bounding_box[\"height\"]/ratio[1]), int(bounding_box[\"left\"]/ratio[0]): int(bounding_box[\"width\"]/ratio[0])]\n",
        "                            \n",
        "                            # increase crop size by 100 in each direction\n",
        "                            # object_crop = frame[int(bounding_box[\"top\"]/ratio[1]-100): int(bounding_box[\"height\"]/ratio[1]+100), int(bounding_box[\"left\"]/ratio[0]-100): int(bounding_box[\"width\"]/ratio[0]+100)]\n",
        "                            \n",
        "                            object_h, object_w = object_crop.shape[:2]\n",
        "                            print(f\"Object width x height: {object_w} x {object_h}\")\n",
        "\n",
        "                            if minimum_crop_size > 0:\n",
        "                                if object_h and object_w >= minimum_crop_size:\n",
        "                                    crop_count += 1\n",
        "                                    encoded_crop = cv2.imencode(\".jpg\", object_crop)[1].tobytes()\n",
        "                                    try:\n",
        "                                        blob_client = output_container_client.get_blob_client(f\"{tag_name}/{crop_name}\")\n",
        "                                        blob_client.upload_blob(encoded_crop, blob_type=\"BlockBlob\")\n",
        "                                        print(f\"Successfully uploaded {crop_name}\")\n",
        "                                        # plt.imshow(object_crop)\n",
        "                                        # plt.show()\n",
        "                                    except Exception as e:\n",
        "                                        print(e)\n",
        "\n",
        "                            else:\n",
        "                                crop_count += 1\n",
        "                                encoded_crop = cv2.imencode(\".jpg\", object_crop)[1].tobytes()\n",
        "                                try:\n",
        "                                    blob_client = output_container_client.get_blob_client(f\"{tag_name}/{crop_name}\")\n",
        "                                    blob_client.upload_blob(encoded_crop, blob_type=\"BlockBlob\")\n",
        "                                    print(f\"Successfully uploaded {crop_name}\")\n",
        "                                    # plt.imshow(object_crop)\n",
        "                                    # plt.show()\n",
        "                                except Exception as e:\n",
        "                                    print(e)\n",
        "\n",
        "else:\n",
        "    print(\"Please select a project type\")\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "44448eee88f2cb51c65ff80ff7e184f1dbe998ecd781b41dd613c0a0f88170d7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
